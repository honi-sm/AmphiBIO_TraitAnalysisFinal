# -*- coding: utf-8 -*-
"""CleanedPythonCode_STRATOS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iN42-OsP4narq_K_eCs8PQ-Dzm9Q3tBA
"""

"""
AmphiBIO Analysis Pipeline

Author: Lia Grace Stratos
Institution: University of Nebraska Omaha
Date: November 8 2025
"""

from google.colab import files
uploadedFiles = files.upload()

import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import shapiro, levene, skew, kurtosis, probplot

sns.set(style="whitegrid")

RAW_FILE = "AmphiBIOv1Raw.xlsx"
OUTPUT_CLEAN = "cleanDataPython.csv"
OUTPUT_SAMPLE = "randomSample_500_python.csv"
RANDOM_SEED = 2025

"""This step just gets the Excel file into Python reliably.
It normalizes weird missing-value patterns ("NA", spaces, etc.) and prints out the shape and first few rows so I can double-check that I didn’t accidentally load something cursed. If the file is missing or empty, it stops immediately.
"""

class RawDatasetLoader:
    """
    Loads the raw AmphiBIO Excel file with basic checks.
    """

    def __init__(self, rawFilePath: str):
        self.rawFilePath = rawFilePath
        self.rawDf: pd.DataFrame | None = None

    def loadData(self) -> pd.DataFrame:
        """Load the raw dataset, handle missing-value codes, and log basic info."""

        if not os.path.exists(self.rawFilePath):
            sys.exit(f"[RawDatasetLoader] Error: File '{self.rawFilePath}' not found in working directory.")

        try:
            rawDf = pd.read_excel(
                self.rawFilePath,
                engine="openpyxl",
                na_values=["NA", "N/A", "na", "NaN", "", " "]
            )
        except Exception as e:
            sys.exit(f"[RawDatasetLoader] Error reading '{self.rawFilePath}': {e}")

        # Basic sanity checks
        if not isinstance(rawDf, pd.DataFrame):
            raise TypeError("[RawDatasetLoader] Loaded file is not a valid pandas DataFrame.")

        if rawDf.shape[0] == 0:
            raise ValueError("[RawDatasetLoader] The file loaded successfully but contains 0 rows.")

        self.rawDf = rawDf

        print("\n[RawDatasetLoader] Loaded file:", self.rawFilePath)
        print("[RawDatasetLoader] Shape (rows, columns):", rawDf.shape)
        print("\n[RawDatasetLoader] First 5 rows:")
        print(rawDf.head())
        print("\n[RawDatasetLoader] Column dtypes before any cleaning:")
        print(rawDf.dtypes)

        return rawDf

"""* remove the taxonomy columns I’m not using for this  project

* check that all the columns I do need actually exist

* lowercase and de-spaghetti the species names (they are all over the place)

* convert numeric columns so Python stops treating them like text

* replace fake-zero NA’s with actual missing values

* filter out species missing body length or clutch size info

* combine habitat indicators into one readable label

* do the same for reproductive mode

* drop any repeated species so each name shows up once

Once this finishes, I end up with ~1480-ish usable rows. That’s the dataset I analyze, as done manually before in excel.
"""

class HabitatCategoryBuilder:
    """
    Static helper that combines habitat indicator columns into a single habitatType label.
    """

    @staticmethod
    def makeHabitatLabel(row: pd.Series) -> str | float:
        parts = []
        if row.get("Fos", 0) == 1:
            parts.append("fossorial")
        if row.get("Ter", 0) == 1:
            parts.append("terrestrial")
        if row.get("Aqu", 0) == 1:
            parts.append("aquatic")
        if row.get("Arb", 0) == 1:
            parts.append("arboreal")
        if len(parts) == 0:
            return np.nan
        return " + ".join(parts)


class ReproductionCategoryBuilder:
    """
    Static helper that maps reproduction indicator columns into a single reproductiveMethod label.
    """

    @staticmethod
    def makeReproductionLabel(row: pd.Series) -> str | float:
        if row.get("Dir", 0) == 1:
            return "direct"
        if row.get("Lar", 0) == 1:
            return "larval"
        if row.get("Viv", 0) == 1:
            return "viviparous"
        return np.nan

class TraitDatasetCleaner:
    """
    Cleans the AmphiBIO trait dataset.

    Steps:
    1. Drop metadata columns not used for analysis.
    2. Confirm that required columns exist.
    3. Normalize species names.
    4. Type cast numeric and binary columns.
    5. Treat zeros as missing where appropriate.
    6. Filter out rows missing key numeric traits.
    7. Build habitatType and reproductiveMethod categories.
    8. Remove duplicate species.
    9. Return a final cleaned DataFrame with standardized column names.
    """

    def __init__(self, rawDf: pd.DataFrame):
        self.rawDf = rawDf.copy()
        self.cleanDf: pd.DataFrame | None = None

        self.numericColumns = ["Body_size_mm", "Litter_size_min_n", "Litter_size_max_n"]
        self.binaryColumns = ["Fos", "Ter", "Aqu", "Arb", "Dir", "Lar", "Viv"]

        self.requiredColumns = [
            "Species", "Fos", "Ter", "Aqu", "Arb",
            "Body_size_mm", "Litter_size_min_n", "Litter_size_max_n",
            "Dir", "Lar", "Viv"
        ]

    def dropMetadataColumns(self) -> None:
        dropCols = ["id", "Order", "Family", "Genus", "OBS"]
        self.rawDf = self.rawDf.drop(columns=[c for c in dropCols if c in self.rawDf.columns])
        print("\n[TraitDatasetCleaner] After dropping metadata columns:")
        print(self.rawDf.columns.tolist())

    def validateRequiredColumns(self) -> None:
        missing = [c for c in self.requiredColumns if c not in self.rawDf.columns]
        if len(missing) > 0:
            sys.exit(f"[TraitDatasetCleaner] Error: Missing required columns: {missing}")
        print("\n[TraitDatasetCleaner] All required columns are present.")

    def normalizeSpeciesNames(self) -> None:
        self.rawDf["Species"] = (
            self.rawDf["Species"]
            .astype(str)
            .str.strip()
            .str.lower()
            .str.replace(r"\s+", " ", regex=True)
        )
        print("\n[TraitDatasetCleaner] Normalized Species names (first 10):")
        print(self.rawDf["Species"].head(10))

    def castNumericAndBinaryColumns(self) -> None:
        print("\n[TraitDatasetCleaner] Numeric dtypes before cast:")
        print(self.rawDf[self.numericColumns].dtypes)

        for col in self.numericColumns:
            self.rawDf[col] = pd.to_numeric(self.rawDf[col], errors="coerce")

        print("\n[TraitDatasetCleaner] Numeric dtypes after cast:")
        print(self.rawDf[self.numericColumns].dtypes)

        print("\n[TraitDatasetCleaner] Binary dtypes before cast:")
        print(self.rawDf[self.binaryColumns].dtypes)

        for col in self.binaryColumns:
            self.rawDf[col] = pd.to_numeric(self.rawDf[col], errors="coerce").astype("float64")

        print("\n[TraitDatasetCleaner] Binary dtypes after cast:")
        print(self.rawDf[self.binaryColumns].dtypes)

    def replaceZerosWithMissing(self) -> None:
        zeroCountBefore = (self.rawDf == 0).sum().sum()
        self.rawDf.replace(0, np.nan, inplace=True)
        zeroCountAfter = (self.rawDf == 0).sum().sum()
        print("\n[TraitDatasetCleaner] Zeros replaced by NaN:", zeroCountBefore - zeroCountAfter)

    def filterMissingNumericTraits(self) -> None:
        beforeCount = len(self.rawDf)
        self.rawDf = self.rawDf[
            self.rawDf["Body_size_mm"].notna()
            & self.rawDf["Litter_size_min_n"].notna()
            & self.rawDf["Litter_size_max_n"].notna()
        ]
        afterCount = len(self.rawDf)

        print("\n[TraitDatasetCleaner] Rows before numeric filter:", beforeCount)
        print("[TraitDatasetCleaner] Rows after numeric filter:", afterCount)
        print("[TraitDatasetCleaner] Rows removed due to missing numeric traits:", beforeCount - afterCount)

    def addDerivedTraitCategories(self) -> None:
        self.rawDf["habitatType"] = self.rawDf.apply(HabitatCategoryBuilder.makeHabitatLabel, axis=1)
        self.rawDf["reproductiveMethod"] = self.rawDf.apply(ReproductionCategoryBuilder.makeReproductionLabel, axis=1)

        print("\n[TraitDatasetCleaner] Unique habitatType values (up to 10):")
        print(self.rawDf["habitatType"].dropna().unique()[:10])

        print("\n[TraitDatasetCleaner] Unique reproductiveMethod values:")
        print(self.rawDf["reproductiveMethod"].dropna().unique())

        beforeCount = len(self.rawDf)
        self.rawDf = self.rawDf[self.rawDf["reproductiveMethod"].notna()]
        afterCount = len(self.rawDf)

        print("\n[TraitDatasetCleaner] Rows removed due to missing reproductiveMethod:", beforeCount - afterCount)

    def removeDuplicateSpecies(self) -> None:
        beforeCount = len(self.rawDf)
        self.rawDf["Species"] = self.rawDf["Species"].str.strip().str.lower()
        self.rawDf = self.rawDf.drop_duplicates(subset="Species", keep="first")
        afterCount = len(self.rawDf)

        print("\n[TraitDatasetCleaner] Duplicate species removed:", beforeCount - afterCount)

    def finalizeCleanDataset(self) -> pd.DataFrame:
        self.rawDf = self.rawDf.rename(columns={
            "Body_size_mm": "bodyLengthMm",
            "Litter_size_min_n": "clutchSizeMinN",
            "Litter_size_max_n": "clutchSizeMaxN"
        })

        self.cleanDf = self.rawDf[[
            "Species", "bodyLengthMm", "clutchSizeMinN", "clutchSizeMaxN",
            "habitatType", "reproductiveMethod"
        ]].reset_index(drop=True)

        print("\n[TraitDatasetCleaner] Final cleaned dataset preview (first 10 rows):")
        print(self.cleanDf.head(10))
        print("[TraitDatasetCleaner] Total retained species:", len(self.cleanDf))

        return self.cleanDf

    def runFullClean(self) -> pd.DataFrame:
        """Convenience method to run the entire cleaning pipeline in order."""
        self.dropMetadataColumns()
        self.validateRequiredColumns()
        self.normalizeSpeciesNames()
        self.castNumericAndBinaryColumns()
        self.replaceZerosWithMissing()
        self.filterMissingNumericTraits()
        self.addDerivedTraitCategories()
        self.removeDuplicateSpecies()
        return self.finalizeCleanDataset()

"""Sometimes I need a fixed subsample, like in this case for the project. Uses a random seed so the exact same 500 species show up every time unless I choose a different seed. Keeps my results reproducible."""

class SpeciesSampler:
    """
    Creates reproducible random samples from the cleaned dataset.
    """

    def __init__(self, cleanDf: pd.DataFrame):
        self.cleanDf = cleanDf

    def sampleSpecies(self, sampleSize: int, randomSeed: int = RANDOM_SEED) -> pd.DataFrame:
        if len(self.cleanDf) < sampleSize:
            raise ValueError(
                f"[SpeciesSampler] Not enough rows to sample. Have {len(self.cleanDf)}, requested {sampleSize}."
            )
        sampleDf = self.cleanDf.sample(n=sampleSize, random_state=randomSeed)
        print(f"\n[SpeciesSampler] Sampled {sampleSize} species using randomSeed={randomSeed}.")
        return sampleDf

"""Some species belong to more than one habitat, so I break those out into separate rows.
This makes the later tests behave correctly because each row now represents one species–habitat combo.

Then I log-transform the traits that are super skewed. Frog clutches go from ~3 to literally thousands, so raw values aren’t really ideal.

If a value is zero or negative, it gets removed. Categorical can’t be log-transformed anyway.
"""

class MultiHabitatExpander:
    """
    Expands species with multiple habitats into one row per habitat,
    and applies log10 transforms to numeric columns.
    """

    def __init__(self, cleanDf: pd.DataFrame):
        self.cleanDf = cleanDf.copy()
        self.expandedDf: pd.DataFrame | None = None

        self.numericColumns = ["bodyLengthMm", "clutchSizeMinN", "clutchSizeMaxN"]
        self.transformedColumns: list[str] = []

    def expandHabitats(self) -> pd.DataFrame:
        habitatSplitSeries = (
            self.cleanDf["habitatType"]
            .astype(str)
            .str.lower()
            .str.replace(" ", "")
            .str.split(r"\+|,|;|/")
        )

        expandedDf = self.cleanDf.copy()
        expandedDf = expandedDf.assign(habitat_split=habitatSplitSeries).explode("habitat_split")

        expandedDf = expandedDf.dropna(subset=["habitat_split"])
        expandedDf["habitat_split"] = expandedDf["habitat_split"].str.strip()
        expandedDf = expandedDf[expandedDf["habitat_split"] != ""]

        replaceMap = {
            "fossorial": "fossorial",
            "fosorial": "fossorial",
            "fossforial": "fossorial",
            "terrestrial": "terrestrial",
            "aquatic": "aquatic",
            "arboreal": "arboreal"
        }

        expandedDf["habitat_split"] = expandedDf["habitat_split"].replace(replaceMap)

        validHabitats = ["fossorial", "terrestrial", "aquatic", "arboreal"]
        expandedDf = expandedDf[expandedDf["habitat_split"].isin(validHabitats)]

        print("\n[MultiHabitatExpander] Expanded habitat counts (one row per species-habitat):")
        print(expandedDf["habitat_split"].value_counts())

        print("\n[MultiHabitatExpander] Example expanded rows:")
        print(expandedDf[["Species", "habitatType", "habitat_split"]].head(10))

        self.expandedDf = expandedDf
        return expandedDf

    def applyLogTransforms(self) -> pd.DataFrame:
        if self.expandedDf is None:
            raise RuntimeError("[MultiHabitatExpander] Call expandHabitats() before applyLogTransforms().")

        for col in self.numericColumns:
            self.expandedDf[col] = pd.to_numeric(self.expandedDf[col], errors="coerce")

        print("\n[MultiHabitatExpander] Invalid numeric values (<= 0) before log10 transform:")
        for col in self.numericColumns:
            invalidCount = (self.expandedDf[col] <= 0).sum()
            print(f"  {col}: invalid count:", invalidCount)
            self.expandedDf = self.expandedDf[self.expandedDf[col] > 0]

        for col in self.numericColumns:
            logCol = f"log_{col}"
            self.expandedDf[logCol] = np.log10(self.expandedDf[col])
            self.transformedColumns.append(logCol)

        print("\n[MultiHabitatExpander] Added log10 transformed columns:")
        print(self.transformedColumns)

        print("\n[MultiHabitatExpander] Summary of transformed columns:")
        print(self.expandedDf[self.transformedColumns].describe())

        return self.expandedDf

"""Check that nothing looks off/normality:

* histograms
* Q-Q plots
* habitat frequency bar charts
* reproductive mode bar charts

If a column doesn’t plot for some reason, the code prints a message instead of blowing up.
"""

class TraitVisualizationSuite:
    """
    Generates histograms, Q-Q plots, and basic categorical distributions
    for the amphibian trait dataset.
    """

    def __init__(self, expandedDf: pd.DataFrame, numericColumns: list[str], transformedColumns: list[str]):
        self.expandedDf = expandedDf
        self.numericColumns = numericColumns
        self.transformedColumns = transformedColumns

        self.colorMapNumeric = {
            "bodyLengthMm": "#4C72B0",
            "clutchSizeMinN": "#55A868",
            "clutchSizeMaxN": "#8172B3",
            "log_bodyLengthMm": "#E58606",
            "log_clutchSizeMinN": "#8EBA42",
            "log_clutchSizeMaxN": "#937860"
        }

        self.prettyNames = {
            "bodyLengthMm": "Body Length (mm)",
            "clutchSizeMinN": "Clutch Size (Minimum)",
            "clutchSizeMaxN": "Clutch Size (Maximum)",
            "log_bodyLengthMm": "Log10 Body Length",
            "log_clutchSizeMinN": "Log10 Clutch Size (Min)",
            "log_clutchSizeMaxN": "Log10 Clutch Size (Max)"
        }

    def plotTraitDistributions(self) -> None:
        """Histogram + Q-Q plot for each raw and log-transformed variable."""
        for col in self.numericColumns + self.transformedColumns:
            fig, axes = plt.subplots(1, 2, figsize=(13, 5))

            histTitle = f"Distribution of {self.prettyNames[col]} (Histogram)"
            qqTitle = f"Normality Check for {self.prettyNames[col]} (Q-Q Plot)"

            sns.histplot(
                self.expandedDf[col],
                bins=30,
                kde=True,
                color=self.colorMapNumeric.get(col, "#888888"),
                ax=axes[0]
            )
            axes[0].set_title(histTitle, fontsize=13, fontweight="bold")
            axes[0].set_xlabel(self.prettyNames[col])
            axes[0].set_ylabel("Frequency")

            probplot(self.expandedDf[col].dropna(), dist="norm", plot=axes[1])
            axes[1].set_title(qqTitle, fontsize=13, fontweight="bold")

            plt.tight_layout()
            plt.show()

    def plotCategoryDistributions(self) -> None:
        """Plots habitat frequencies and reproductive method frequencies."""

        plt.figure(figsize=(8, 5))
        sns.countplot(
            x=self.expandedDf["habitat_split"],
            order=["fossorial", "terrestrial", "aquatic", "arboreal"],
            palette=["#4C72B0", "#55A868", "#8172B3", "#E58606"]
        )
        plt.title(
            "Distribution of Habitat Types (Expanded)\nEach species counted once per habitat",
            fontsize=14,
            fontweight="bold"
        )
        plt.xlabel("Habitat Type")
        plt.ylabel("Frequency")
        plt.tight_layout()
        plt.show()

        if "reproductiveMethod" in self.expandedDf.columns:
            plt.figure(figsize=(8, 5))
            sns.countplot(
                x=self.expandedDf["reproductiveMethod"],
                palette=["#4C72B0", "#55A868", "#8172B3", "#E58606"]
            )
            plt.title(
                "Distribution of Reproductive Modes",
                fontsize=14,
                fontweight="bold"
            )
            plt.xlabel("Reproductive Method")
            plt.ylabel("Frequency")
            plt.tight_layout()
            plt.show()

"""Before doing any version of ANOVA, I check:

* Shapiro-Wilk normality
* skew
* kurtosis
* normality within each habitat group
* Levene’s test for equal variances

Confirms what I already know: the raw traits are definitely not normal, and the log-transformed ones are… closer.
"""

class AnovaAssumptionChecker:
    """
    Checks ANOVA assumptions:
    1. Overall normality (Shapiro, skew, kurtosis) for raw and log traits.
    2. Normality within each habitat group.
    3. Homogeneity of variance via Levene's test.
    """

    def __init__(self, expandedDf: pd.DataFrame, numericColumns: list[str], transformedColumns: list[str]):
        self.expandedDf = expandedDf
        self.numericColumns = numericColumns
        self.transformedColumns = transformedColumns

        self.prettyNames = {
            "bodyLengthMm": "Body Length (mm)",
            "clutchSizeMinN": "Clutch Size (Minimum)",
            "clutchSizeMaxN": "Clutch Size (Maximum)",
            "log_bodyLengthMm": "Log10 Body Length",
            "log_clutchSizeMinN": "Log10 Clutch Size (Min)",
            "log_clutchSizeMaxN": "Log10 Clutch Size (Max)"
        }

    def checkOverallNormality(self) -> pd.DataFrame:
        results = []

        for col in self.numericColumns + self.transformedColumns:
            values = self.expandedDf[col].dropna()
            if len(values) >= 3:
                w, p = shapiro(values)
                results.append({
                    "Variable": self.prettyNames[col],
                    "N": len(values),
                    "Skewness": round(skew(values), 3),
                    "Kurtosis": round(kurtosis(values), 3),
                    "ShapiroW": round(w, 4),
                    "pValue": round(p, 6),
                    "ApproxNormal": "Yes" if p > 0.05 else "No"
                })

        overallDf = pd.DataFrame(results)
        print("\n[AnovaAssumptionChecker] Overall normality (raw and log):")
        print(overallDf)

        print("\nNotes:")
        print("- pValue > 0.05 means we do not reject normality.")
        print("- Large absolute skew or kurtosis suggests non-normality.")

        return overallDf

    def checkNormalityByHabitat(self) -> pd.DataFrame:
        habitatResults = []

        for habitat, groupDf in self.expandedDf.groupby("habitat_split"):
            for col in self.numericColumns + self.transformedColumns:
                values = groupDf[col].dropna()
                if len(values) >= 3:
                    w, p = shapiro(values)
                    habitatResults.append({
                        "Habitat": habitat,
                        "Variable": self.prettyNames[col],
                        "N": len(values),
                        "Skewness": round(skew(values), 3),
                        "Kurtosis": round(kurtosis(values), 3),
                        "ShapiroW": round(w, 4),
                        "pValue": round(p, 6),
                        "ApproxNormal": "Yes" if p > 0.05 else "No"
                    })

        habitatDf = pd.DataFrame(habitatResults)
        print("\n[AnovaAssumptionChecker] Normality tests by habitat:")
        print(habitatDf)

        print("\nNotes:")
        print("- Some habitats may have small sample sizes; interpret Shapiro carefully.")

        return habitatDf

    def checkLeveneHomogeneity(self) -> pd.DataFrame:
        leveneResults = []

        for col in self.numericColumns + self.transformedColumns:
            groups = [
                g[col].dropna().values
                for _, g in self.expandedDf.groupby("habitat_split")
                if len(g[col].dropna()) >= 3
            ]
            if len(groups) > 1:
                stat, p = levene(*groups)
                leveneResults.append({
                    "Variable": self.prettyNames[col],
                    "FStatistic": round(stat, 4),
                    "pValue": round(p, 6),
                    "EqualVariances": "Yes" if p > 0.05 else "No"
                })

        leveneDf = pd.DataFrame(leveneResults)
        print("\n[AnovaAssumptionChecker] Levene homogeneity of variance results:")
        print(leveneDf)

        print("\nNotes:")
        print("- pValue > 0.05 means equal variances are a reasonable assumption.")
        print("- If pValue <= 0.05, consider Welch ANOVA or nonparametric tests.")

        return leveneDf

"""Welch ANOVA doesn’t care about equal variances and handles skew better, but I still check:
* group sizes
* skew within each habitat
* how different the variances are

It returns a simple “yes,” “no,”
"""

class WelchSuitabilityChecker:
    """
    Checks if Welch ANOVA is reasonable for the log-transformed traits.
    Focuses on:
    - Group sample sizes
    - Group-level skewness
    - Variance differences (allowed by Welch, but reported)
    """

    def __init__(self, expandedDf: pd.DataFrame, transformedColumns: list[str], prettyNames: dict):
        self.expandedDf = expandedDf
        self.transformedColumns = transformedColumns
        self.prettyNames = prettyNames

    def checkWelchSuitability(self) -> pd.DataFrame:
        grouped = {hab: g for hab, g in self.expandedDf.groupby("habitat_split")}
        results = []

        for col in self.transformedColumns:
            rowResult = {"Variable": self.prettyNames[col]}

            groupNs = {hab: len(grouped[hab][col].dropna()) for hab in grouped}
            rowResult["NsPerHabitat"] = groupNs
            rowResult["HasSmallGroups"] = any(n < 3 for n in groupNs.values())

            groupSkews = {
                hab: round(skew(grouped[hab][col].dropna()), 3)
                for hab in grouped
                if len(grouped[hab][col].dropna()) >= 3
            }
            rowResult["SkewnessPerHabitat"] = groupSkews
            rowResult["HasHighSkew"] = any(abs(s) > 2 for s in groupSkews.values())

            groupVars = {
                hab: round(grouped[hab][col].dropna().var(), 4)
                for hab in grouped
                if len(grouped[hab][col].dropna()) > 0
            }
            rowResult["Variances"] = groupVars

            if rowResult["HasSmallGroups"]:
                rowResult["WelchOK"] = "No (some habitats have N < 3)"
            elif rowResult["HasHighSkew"]:
                rowResult["WelchOK"] = "Maybe (some habitats have strong skew)"
            else:
                rowResult["WelchOK"] = "Yes"

            results.append(rowResult)

        welchDf = pd.DataFrame(results)
        print("\n[WelchSuitabilityChecker] Welch ANOVA suitability:")
        print(welchDf)

        print("\nNotes:")
        print("- Welch ANOVA does not require equal variances.")
        print("- Extremely small groups or extreme skew make interpretation harder.")

        return welchDf

"""This part tells me, for each variable:

* whether classical ANOVA is okay
* whether Welch is okay
* whether Kruskal–Wallis is okay
* Pearson vs. Spearman correlation
* whether linear regression is reasonable on the log-transformed values

It’s basically my quick reference so I'm not guessing every time.
"""

class StatisticalSuitabilityReport:
    """
    Summarizes which tests are reasonable for each log-transformed trait:
    - Classical ANOVA
    - Welch ANOVA
    - Kruskal-Wallis
    - Chi-Square (for categorical analyses, noted only)
    - Pearson and Spearman correlation
    - Linear regression
    """

    def __init__(self, expandedDf: pd.DataFrame, transformedColumns: list[str]):
        self.expandedDf = expandedDf
        self.transformedColumns = transformedColumns
        self.groups = {hab: g for hab, g in self.expandedDf.groupby("habitat_split")}
        self.habitats = list(self.groups.keys())

    def isNormal(self, values: pd.Series) -> bool | None:
        if len(values) < 3:
            return None
        _, p = shapiro(values)
        return p > 0.05

    def hasModerateSkew(self, values: pd.Series) -> bool:
        return abs(skew(values)) < 2

    def hasEqualVariances(self, col: str) -> bool:
        arrays = [g[col].dropna().values for g in self.groups.values()]
        stat, p = levene(*arrays)
        return p > 0.05

    def allGroupsHaveEnoughN(self, col: str) -> bool:
        return all(len(self.groups[h][col].dropna()) >= 3 for h in self.habitats)

    def buildReport(self) -> pd.DataFrame:
        reportRows = []

        for col in self.transformedColumns:
            values = self.expandedDf[col].dropna()

            anovaNormal = all(
                self.isNormal(self.groups[h][col].dropna())
                for h in self.habitats
                if len(self.groups[h][col].dropna()) >= 3
            )
            anovaEqualVar = self.hasEqualVariances(col)
            anovaN = self.allGroupsHaveEnoughN(col)

            classicalAnovaStatus = "Suitable" if (anovaN and anovaNormal and anovaEqualVar) else "Not suitable"

            welchSkewOK = all(
                self.hasModerateSkew(self.groups[h][col].dropna())
                for h in self.habitats
                if len(self.groups[h][col].dropna()) >= 3
            )
            welchN = anovaN
            welchStatus = "Suitable" if (welchN and welchSkewOK) else "Possible with caution"

            kruskalStatus = "Suitable" if (welchN and welchSkewOK) else "Caution (shapes differ a lot)"

            chiStatus = "Suitable (for categorical-categorical tests, not directly this variable)"

            pearsonStatus = "Suitable" if self.isNormal(values) else "Use Spearman instead"
            spearmanStatus = "Suitable"

            regressionSkewOK = self.hasModerateSkew(values)
            regressionStatus = "Suitable (log transform improved skew)" if regressionSkewOK else "Residual skew likely"

            reportRows.append({
                "Variable": col,
                "ClassicalANOVA": classicalAnovaStatus,
                "WelchANOVA": welchStatus,
                "KruskalWallis": kruskalStatus,
                "ChiSquare": chiStatus,
                "PearsonCorrelation": pearsonStatus,
                "SpearmanCorrelation": spearmanStatus,
                "LinearRegression": regressionStatus
            })

        suitabilityDf = pd.DataFrame(reportRows)
        print("\n[StatisticalSuitabilityReport] Statistical test suitability summary:")
        print(suitabilityDf)

        print("\nNotes:")
        print("- Classical ANOVA: needs normality by group, equal variances, and n >= 3.")
        print("- Welch ANOVA: more robust to variance differences; still needs reasonable group sizes.")
        print("- Kruskal-Wallis: nonparametric alternative using ranks.")
        print("- Pearson: best with roughly normal variables; otherwise Spearman.")
        print("- Regression: log transforms usually help but residuals should still be checked.")

        return suitabilityDf

class AmphiBIOAnalysisPipeline:
    """
    High-level orchestrator for the full AmphiBIO analysis:
    1. Load raw data.
    2. Clean traits.
    3. Save cleaned dataset and a random sample.
    4. Expand habitats and apply log transforms.
    5. Run visualizations.
    6. Check ANOVA assumptions.
    7. Assess Welch suitability.
    8. Generate a general statistical suitability report.
    """

    def __init__(self, rawFilePath: str = RAW_FILE):
        self.rawFilePath = rawFilePath
        self.rawDf: pd.DataFrame | None = None
        self.cleanDf: pd.DataFrame | None = None
        self.sampleDf: pd.DataFrame | None = None
        self.expandedDf: pd.DataFrame | None = None

        self.numericColumns = ["bodyLengthMm", "clutchSizeMinN", "clutchSizeMaxN"]
        self.transformedColumns: list[str] = []

    def runCleaningAndSampling(self) -> None:
        loader = RawDatasetLoader(self.rawFilePath)
        self.rawDf = loader.loadData()

        cleaner = TraitDatasetCleaner(self.rawDf)
        self.cleanDf = cleaner.runFullClean()

        self.cleanDf.to_csv(OUTPUT_CLEAN, index=False)
        print(f"\n[AmphiBIOAnalysisPipeline] Saved cleaned dataset to {OUTPUT_CLEAN}")

        sampler = SpeciesSampler(self.cleanDf)
        self.sampleDf = sampler.sampleSpecies(500, randomSeed=RANDOM_SEED)
        self.sampleDf.to_csv(OUTPUT_SAMPLE, index=False)
        print(f"[AmphiBIOAnalysisPipeline] Saved 500-species random sample to {OUTPUT_SAMPLE}")

    def runExpansionAndTransforms(self) -> None:
        if self.cleanDf is None:
            raise RuntimeError("[AmphiBIOAnalysisPipeline] Clean dataset is not available. Run runCleaningAndSampling() first.")

        expander = MultiHabitatExpander(self.cleanDf)
        self.expandedDf = expander.expandHabitats()
        self.expandedDf = expander.applyLogTransforms()
        self.transformedColumns = expander.transformedColumns

    def runVisualizations(self) -> None:
        if self.expandedDf is None:
            raise RuntimeError("[AmphiBIOAnalysisPipeline] Expanded dataset not available. Run runExpansionAndTransforms() first.")

        vizSuite = TraitVisualizationSuite(
            expandedDf=self.expandedDf,
            numericColumns=self.numericColumns,
            transformedColumns=self.transformedColumns
        )
        vizSuite.plotTraitDistributions()
        vizSuite.plotCategoryDistributions()

    def runAnovaAssumptionChecks(self) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        if self.expandedDf is None:
            raise RuntimeError("[AmphiBIOAnalysisPipeline] Expanded dataset not available. Run runExpansionAndTransforms() first.")

        checker = AnovaAssumptionChecker(
            expandedDf=self.expandedDf,
            numericColumns=self.numericColumns,
            transformedColumns=self.transformedColumns
        )

        overallNormalityDf = checker.checkOverallNormality()
        habitatNormalityDf = checker.checkNormalityByHabitat()
        leveneDf = checker.checkLeveneHomogeneity()

        return overallNormalityDf, habitatNormalityDf, leveneDf

    def runWelchSuitability(self) -> pd.DataFrame:
        if self.expandedDf is None:
            raise RuntimeError("[AmphiBIOAnalysisPipeline] Expanded dataset not available. Run runExpansionAndTransforms() first.")

        prettyNamesLog = {
            "log_bodyLengthMm": "Log10 Body Length",
            "log_clutchSizeMinN": "Log10 Clutch Size (Min)",
            "log_clutchSizeMaxN": "Log10 Clutch Size (Max)"
        }

        welchChecker = WelchSuitabilityChecker(
            expandedDf=self.expandedDf,
            transformedColumns=self.transformedColumns,
            prettyNames=prettyNamesLog
        )
        return welchChecker.checkWelchSuitability()

    def runSuitabilityReport(self) -> pd.DataFrame:
        if self.expandedDf is None:
            raise RuntimeError("[AmphiBIOAnalysisPipeline] Expanded dataset not available. Run runExpansionAndTransforms() first.")

        reportBuilder = StatisticalSuitabilityReport(
            expandedDf=self.expandedDf,
            transformedColumns=self.transformedColumns
        )
        return reportBuilder.buildReport()

"""This includes descriptive stats:

* means, medians, quartiles, variance
* same stats split by habitat
* Pearson and Spearman correlation matrices
* outlier flags using the IQR rule
"""

class DescriptiveStatsSuite:
    """
    Computes descriptive statistics for the amphibian dataset.
    Includes:
      1. Overall descriptive stats (mean, SD, min, max, quartiles)
      2. Grouped stats by habitat
      3. Correlation matrices (Pearson & Spearman)
      4. Outlier detection using IQR rule
    """

    def __init__(self, expandedDf: pd.DataFrame, numericColumns: list[str], transformedColumns: list[str]):
        self.expandedDf = expandedDf
        self.numericColumns = numericColumns
        self.transformedColumns = transformedColumns
        self.allNumericColumns = numericColumns + transformedColumns

    # -------------------------------------------------------
    def computeOverallStats(self) -> pd.DataFrame:
        print("\n[DescriptiveStatsSuite] Computing overall descriptive statistics...")
        statsDf = self.expandedDf[self.allNumericColumns].describe().T
        statsDf["variance"] = self.expandedDf[self.allNumericColumns].var()
        statsDf["skewness"] = self.expandedDf[self.allNumericColumns].skew()
        statsDf["kurtosis"] = self.expandedDf[self.allNumericColumns].kurtosis()

        print("\n[DescriptiveStatsSuite] Overall Descriptive Stats:")
        print(statsDf)
        return statsDf

    # -------------------------------------------------------
    def computeGroupedStats(self) -> pd.DataFrame:
        print("\n[DescriptiveStatsSuite] Computing grouped descriptive statistics by habitat_split...")

        groupedStats = (
            self.expandedDf
            .groupby("habitat_split")[self.allNumericColumns]
            .agg(["count", "mean", "std", "min", "max", "median"])
        )

        print("\n[DescriptiveStatsSuite] Grouped Stats (per habitat):")
        print(groupedStats)
        return groupedStats

    # -------------------------------------------------------
    def computeCorrelations(self) -> tuple[pd.DataFrame, pd.DataFrame]:
        print("\n[DescriptiveStatsSuite] Computing Pearson and Spearman correlation matrices...")

        pearsonCorr = self.expandedDf[self.allNumericColumns].corr(method="pearson")
        spearmanCorr = self.expandedDf[self.allNumericColumns].corr(method="spearman")

        print("\n[DescriptiveStatsSuite] Pearson Correlation Matrix:")
        print(pearsonCorr)

        print("\n[DescriptiveStatsSuite] Spearman Correlation Matrix:")
        print(spearmanCorr)

        return pearsonCorr, spearmanCorr

    # -------------------------------------------------------
    def computeOutlierFlags(self) -> pd.DataFrame:
        print("\n[DescriptiveStatsSuite] Identifying potential outliers using the IQR rule...")

        outlierDf = pd.DataFrame(index=self.expandedDf.index)

        for col in self.allNumericColumns:
            Q1 = self.expandedDf[col].quantile(0.25)
            Q3 = self.expandedDf[col].quantile(0.75)
            IQR = Q3 - Q1
            lowerBound = Q1 - 1.5 * IQR
            upperBound = Q3 + 1.5 * IQR

            outlierDf[f"{col}_isOutlier"] = (
                (self.expandedDf[col] < lowerBound) | (self.expandedDf[col] > upperBound)
            )

        print("\n[DescriptiveStatsSuite] Outlier Flags (first 10 rows):")
        print(outlierDf.head(10))

        print("\n[DescriptiveStatsSuite] Total outliers per variable:")
        print(outlierDf.sum())

        return outlierDf

    # -------------------------------------------------------
    def runFullDescriptiveAnalysis(self) -> dict:
        print("\n[DescriptiveStatsSuite] Running full descriptive statistics suite...")

        overall = self.computeOverallStats()
        grouped = self.computeGroupedStats()
        pearsonCorr, spearmanCorr = self.computeCorrelations()
        outliers = self.computeOutlierFlags()

        return {
            "overallStats": overall,
            "groupedStats": grouped,
            "pearsonCorr": pearsonCorr,
            "spearmanCorr": spearmanCorr,
            "outlierFlags": outliers
        }

# Cell Y: Run descriptive statistics safely

# 1. Make sure the main pipeline object exists
try:
    amphibioPipeline
except NameError:
    print("[Cell Y] amphibioPipeline not found. Creating a new one and running the full pipeline...")
    amphibioPipeline = AmphiBIOAnalysisPipeline(rawFilePath=RAW_FILE)
    amphibioPipeline.runCleaningAndSampling()
    amphibioPipeline.runExpansionAndTransforms()

# 2. Make sure expandedDf is available
if amphibioPipeline.expandedDf is None:
    print("[Cell Y] expandedDf is None. Running habitat expansion and log transforms...")
    amphibioPipeline.runExpansionAndTransforms()

# 3. Quick sanity print
print("\n[Cell Y] expandedDf shape:", amphibioPipeline.expandedDf.shape)
print("[Cell Y] numericColumns:", amphibioPipeline.numericColumns)
print("[Cell Y] transformedColumns:", amphibioPipeline.transformedColumns)

# 4. Run the descriptive statistics suite
descStats = DescriptiveStatsSuite(
    expandedDf=amphibioPipeline.expandedDf,
    numericColumns=amphibioPipeline.numericColumns,
    transformedColumns=amphibioPipeline.transformedColumns
)

descriptiveResults = descStats.runFullDescriptiveAnalysis()

# 5. Optional: unpack for easier access later
overallStatsDf = descriptiveResults["overallStats"]
groupedStatsDf = descriptiveResults["groupedStats"]
pearsonCorrDf = descriptiveResults["pearsonCorr"]
spearmanCorrDf = descriptiveResults["spearmanCorr"]
outlierFlagsDf = descriptiveResults["outlierFlags"]

print("\n[Cell Y] Descriptive statistics run complete.")

# Create and run the full analysis pipeline

amphibioPipeline = AmphiBIOAnalysisPipeline(rawFilePath=RAW_FILE)

# Step 1: Cleaning and sampling
amphibioPipeline.runCleaningAndSampling()

# Step 2: Habitat expansion and log transforms
amphibioPipeline.runExpansionAndTransforms()

# Step 3: Visualizations (optional; will show multiple plots)
amphibioPipeline.runVisualizations()

# Step 4: ANOVA assumption checks
overallNormalityDf, habitatNormalityDf, leveneResultsDf = amphibioPipeline.runAnovaAssumptionChecks()

# Step 5: Welch ANOVA suitability check
welchSuitabilityDf = amphibioPipeline.runWelchSuitability()

# Step 6: General statistical suitability report
suitabilitySummaryDf = amphibioPipeline.runSuitabilityReport()